---
title: "Ciência de Dados"
output:
  html_document:
    df_print: paged
knit: (function(input_file, encoding) { out_dir <- 'docs'; rmarkdown::render(input_file,
  encoding=encoding, output_file=file.path(dirname(input_file), out_dir, 'cienciadados.html'))})
---

O objetivo desse notebook é aplicar a Linguagem R em Ciência de Dados na resolução de problemas de forma não supervisionada e supervisionada. 

## Agrupando dados

Aplicando o KMeans com 2:10 centroides para a base Iris:

```{r}
par(mfrow=c(2,2))
plot = sapply(c(2,3,5,7), function(i) {
  centroides = kmeans(iris[,1:4], centers=i, iter.max=500, nstart=30, algorithm="MacQueen")
  plot(iris[,3:4], col=centroides$cluster)  
})
```

## Comprimindo Imagens

Um experimento interessante é a tarefa de comprimir imagens utilizando o algoritmo de agrupamento KMeans. Para a compressão, o número de cores precisa ser reduzido. A redução é feita através do agrupamento das cores e seleção daquelas mais frequentes. Para isso precisamos carregar a blbioteca imager:   

```{r}
if(!require('imager')) {
  install.packages('imager')
}

require('imager')
```

Algumas imagens são pré-carregadas pelo pacote como a imagem de barcos:

```{r}
plot(boats, axes=FALSE)
```

Vamos salvar a imagem original em um arquivo e imprimir seu tamanho em disco:

```{r}
save(boats, file='boats1.zip')
cat('Tamanho da imagem original:', file.info("boats1.zip")$size, 'bytes')
```

Podemos extrair informações sobre o objeto da seguinte forma:

```{r}
boats
dim(boats)
```

Gerando o data.frame da imagem boats:

```{r}
image = as.data.frame(boats)
# cabecalho do data.frame
head(image)
```

Podemos deixar a imagem borrada:

```{r}
plot(isoblur(boats, 10), axes=FALSE)
```

Podemos deixar a imagem em escala de cinza:

```{r}
plot(grayscale(boats), axes=FALSE)
```

Podemos remover a cor vermelha da imagem:

```{r}
test_image = image
test_image[test_image$cc == 1, 'value'] = 0
test_image = as.cimg(test_image)
plot(test_image, axes=FALSE)
```

Podemos separar as cores RGB da imagem da seguinte forma: 

```{r}
red = image[image$cc == 1, 'value']
blue = image[image$cc == 2, 'value']
green = image[image$cc == 3, 'value']
```

Reconstruindo a base de dados em um data.frame:

```{r}
data = data.frame(red, blue, green)
```

Aplicando o KMeans com 10 centroides e 500 iterações máximas:

```{r}
centroides = kmeans(data, centers=10, iter.max=500, nstart=30, algorithm="MacQueen")
summary(centroides)
```

Adicionando os centroides encontrados em data:

```{r}
data[,"cluster"] = centroides$cluster
head(data)
```

O próximo passo é reconstruir a imagem utilizando as cores identificadas como centroides. A imagem será reconstrída com as 10 cores principais:

```{r}
new_data = do.call('rbind',
  lapply(1:nrow(data), function(i) {
    centroides$centers[data[i,4],]
  })
)
```

Plotando a nova imagem:

```{r}
new_image = image
new_image$value = c(new_data[,1], new_data[,2], new_data[,3])
new_image = as.cimg(new_image)
plot(new_image, axes=FALSE)
```

Vamos salvar a nova imagem em um arquivo e imprimir o seu tamanho comprimido em disco:

```{r}
save(new_image, file='boats2.zip')
cat('Tamanho da imagem comprimida:', file.info("boats2.zip")$size, 'bytes')
```

Antes de continuar com o experimento, vamos construir a função imageGenerator que recebe uma imagem e o número de centroides e retorna uma nova imagem comprimida:

```{r}
imageGenerator <- function(image, centers) {
  
  image = as.data.frame(image)
  red = image[image$cc == 1, 'value']
  blue = image[image$cc == 2, 'value']
  green = image[image$cc == 3, 'value']
  data = data.frame(red, blue, green)
  
  centroides = kmeans(data, centers, iter.max=500, nstart=30, algorithm="MacQueen")
  data[,"cluster"] = centroides$cluster

  new_data = do.call('rbind',
    lapply(1:nrow(data), function(i) {
      centroides$centers[data[i,4],]
    })
  )
  
  new_image = image
  new_image$value = c(new_data[,1], new_data[,2], new_data[,3])
  as.cimg(new_image)
}
```

Usando a função imageGenerator podemos gerar diferentes imagens com diferentes números de cores. Segue o resultado do experimento para 10, 32, 64 e 128 cores:

```{r}
par(mfrow=c(2,2))
plot = sapply(c(10, 32, 64, 128), function(i) {
  plot(imageGenerator(boats, i), axes=FALSE)
})
```

## Classificando dígitos

Um experimento interessante é a tarefa de classificar dígitos utilizando o algoritmo de Redes Neurais Artificiais. Para isso precisamos relizar os experimentos em duas fases: (1) treinar a rede para reconhecer os dígitos e (2) utilizá-la para classificar novos dígitos. Para isso precisamos carregar as blbiotecas tensorflow e keras:  

```{r}
if(!require('tensorflow')) {
  install.packages('tensorflow')
}

require('tensorflow')
```

```{r}
if(!require('keras')) {
  install.packages('keras')
}

require('keras')
```

A base de dados de dígitos se chama MNIST e pode ser carregada com a função dataset_mnist. Ela contém 60000 amostras dos dígitos 0 até 9. Cada dígito é uma imagem de 28 x 28 pixels. 

```{r}
mnist = dataset_mnist()
```

A função show_digit recebe um dígito na forma de uma matriz 28 x 28 e imprime o dígito na forma de imagem. 

```{r}
show_digit <- function(numero) {
  image(numero[1:28,28:1])
}
```

Podemos utilizar a função show_digit para plotar uma amostra de todos os dígitos presentes na base de dados: 

```{r}
par(mfrow=c(2,5))
plot = lapply(0:9, function(i) {
  aux = mnist$train$x[mnist$train$y == i,,][1,,]
  show_digit(t(aux))
})
```

Normalizando os dados de treinamento e teste:

```{r}
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
```

Construindo uma Rede Neural Artificial usando Keras. Essa rede é composta de uma camanda de entrada flatten (achatada de 28 x 28 neurônios), uma camada oculta dense (128 neurônios com função de ativação linear) e uma camada de saída dense (10 neurônios com função de ativação exponencial normalizada) 

```{r}
modelo = keras_model_sequential() %>% 
  layer_flatten(input_shape = c(28, 28)) %>% 
  layer_dense(units = 128, activation = "relu") %>% 
  layer_dense(10, activation = "softmax")
```

Informações sobre a Rede Neural Artificial:

```{r}
summary(modelo)
```

Ainda precisamos definir o algoritmo de treinamento, a função de avaliação e a medida de desempenho que vamos utilizar. Podemos fazer isso com a função compile:

```{r}
modelo %>% 
  compile(
    loss = "sparse_categorical_crossentropy",
    optimizer = "adam",
    metrics = "accuracy"
  )
```

Agora podemos treinar a rede:

```{r}
historico = modelo %>% 
  fit(
    x = mnist$train$x, y = mnist$train$y,
    epochs = 10,
    validation_split = 0.3,
    verbose = 2
  )
```

É interessante medir o erro e o desempenho da rede ao longo das épocas no conjunto de treinamento e validação.

```{r}
plot(historico)
```

Agora que sabemos que a rede aprendeu a reconhecer dígitos, podemos verificar seu desempenho no conjunto de teste. Para isso precisamos aplicar essas amostras nunca antes vistas na entrada da rede e obter os valores de saída: 

```{r}
predicao <- predict(modelo, mnist$test$x)
head(predicao)
```

Cada neurônio de saída da rede irá retornar um valor. Aquele neurônio com maior valor deve indicar a classe com maior proababilidade:

```{r}
predicao = apply(predicao, 1, which.max)
predicao[1:6]
```

Calculando o desempenho da rede no conjunto de teste:

```{r}
matriz = table(mnist$test$y, predicao)
acuracia = sum(diag(matriz))/sum(matriz)
cat('A acurácia do modelo no conjunto de teste eh:', acuracia)
```
